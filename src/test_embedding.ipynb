{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "from typing import Any, NamedTuple, Optional\n",
    "from typing import Tuple\n",
    "import time\n",
    "\n",
    "from google.api_core.exceptions import InvalidArgument\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import vertexai\n",
    "from numpy import floating\n",
    "from vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\n",
    "\n",
    "DATA_FOLDER = Path(\"../data\")\n",
    "INPUT_FOLDER = DATA_FOLDER / \"query_eval\"\n",
    "RESULT_TASKTYPE_FOLDER = (\n",
    "    DATA_FOLDER / \"embedding_eval_task_type\"\n",
    ")  # this contains the API call with type (QUERY or DOCUMENT)\n",
    "REGION = \"europe-west3\"\n",
    "PROJECT_ID = \"adesso-gcc-rtl-uc4\"\n",
    "\n",
    "\n",
    "class EmbeddingModelConfig(NamedTuple):\n",
    "    name: str\n",
    "    task_type: list[bool]\n",
    "\n",
    "\n",
    "EMBEDDING_MODELS = [\n",
    "    EmbeddingModelConfig(\"textembedding-gecko@001\", [False]),\n",
    "    EmbeddingModelConfig(\"textembedding-gecko@002\", [True, False]),\n",
    "    EmbeddingModelConfig(\"textembedding-gecko@003\", [True, False]),\n",
    "    EmbeddingModelConfig(\"textembedding-gecko-multilingual@001\", [True, False]),\n",
    "]\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VertexAIEmbeddingModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 60\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_distance\u001b[39m(document: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m], query_embedding: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m floating[Any]:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(np\u001b[38;5;241m.\u001b[39marray(document) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39marray(query_embedding))\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_text_embedding_df\u001b[39m(\n\u001b[1;32m     58\u001b[0m     docs: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m     59\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m---> 60\u001b[0m     embedding_models: \u001b[38;5;28mlist\u001b[39m[\u001b[43mVertexAIEmbeddingModel\u001b[49m],\n\u001b[1;32m     61\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m     62\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m     63\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m docs\n",
      "\u001b[0;31mNameError\u001b[0m: name 'VertexAIEmbeddingModel' is not defined"
     ]
    }
   ],
   "source": [
    "def load_csv(file: Path, col_name: str) -> list[str]:\n",
    "    \"\"\"Load a CSV file and return a list of values from the given column.\"\"\"\n",
    "    with open(file, newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        return [\n",
    "            row[col_name].replace(\"ChromaDB:\\n\", \"\") for row in reader if row[col_name]\n",
    "        ]\n",
    "\n",
    "\n",
    "def rate_limit(max_per_minute):\n",
    "    \"\"\"Yield a generator that will pause for a calculated amount of time\n",
    "    to avoid exceeding the rate limit.\"\"\"\n",
    "    period = 60 / max_per_minute\n",
    "    while True:\n",
    "        before = time.time()\n",
    "        yield\n",
    "        after = time.time()\n",
    "        elapsed = after - before\n",
    "        sleep_time = max(0, period - elapsed)\n",
    "        if sleep_time > 0:\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "\n",
    "def encode_texts_to_embeddings(\n",
    "    docs: list[str | TextEmbeddingInput],\n",
    "    embedding_model: TextEmbeddingModel,\n",
    "    use_task_type: bool,\n",
    "    instances_per_batch: int = 5,\n",
    "    requests_per_minute: int = 100,\n",
    ") -> list[Optional[list[float]]]:\n",
    "    \"\"\"Get embeddings for a list of texts using the given embedding model.\n",
    "    use_task_type will use the task type parameter in the API call.\"\"\"\n",
    "\n",
    "    if use_task_type:\n",
    "        docs = [\n",
    "            TextEmbeddingInput(text=sentence, task_type=\"RETRIEVAL_DOCUMENT\")  # type: ignore\n",
    "            for sentence in docs\n",
    "        ]\n",
    "    try:\n",
    "        embeddings = []\n",
    "        limiter = rate_limit(requests_per_minute)\n",
    "        while docs:\n",
    "            # Working in batches because the API accepts maximum 5\n",
    "            # documents per request to get embeddings\n",
    "            head, docs = (\n",
    "                docs[:instances_per_batch],\n",
    "                docs[instances_per_batch:],\n",
    "            )\n",
    "            chunk = embedding_model.get_embeddings(head)\n",
    "            embeddings.extend(chunk)\n",
    "            next(limiter)\n",
    "        return [embedding.values for embedding in embeddings]\n",
    "    except Exception as e:\n",
    "        print(f\"Error while getting embeddings: {e}\")\n",
    "        return [None for _ in range(len(docs))]\n",
    "\n",
    "\n",
    "def get_distance(document: list[float], query_embedding: list[float]) -> floating[Any]:\n",
    "    \"\"\"Gets the L2 distance between the document and the query embedding.\"\"\"\n",
    "    return np.linalg.norm(np.array(document) - np.array(query_embedding))\n",
    "\n",
    "\n",
    "def get_text_embedding_df(\n",
    "    docs: list[str],\n",
    "    query: str,\n",
    "    embedding_models: list[EmbeddingModelConfig],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Get the embeddings for the given documents and query using the given embedding models.\n",
    "    Then calculate the distance between the query and each document embedding.\"\"\"\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df[\"result\"] = docs\n",
    "\n",
    "    for embedding_model in embedding_models:\n",
    "        for use_task_type in embedding_model.task_type:\n",
    "            try:\n",
    "                embedding_model_name = \"\".join(embedding_model.name.split(\"-\")[1:])\n",
    "                task_type_infix = \"_tasktype\" if use_task_type else \"\"\n",
    "                embedding_model_desc = f\"{embedding_model_name}{task_type_infix}\"\n",
    "\n",
    "                model = TextEmbeddingModel.from_pretrained(embedding_model.name)\n",
    "\n",
    "                # get query embedding\n",
    "                if embedding_model.task_type:\n",
    "                    query_textinput = TextEmbeddingInput(\n",
    "                        text=query, task_type=\"RETRIEVAL_QUERY\"\n",
    "                    )  # type: ignore\n",
    "                    query_embedding = model.get_embeddings([query_textinput])[0].values\n",
    "                else:\n",
    "                    query_embedding = model.get_embeddings([query])[0].values\n",
    "\n",
    "                # get text embeddings\n",
    "                df[embedding_model_desc] = encode_texts_to_embeddings(\n",
    "                    docs,  # type: ignore\n",
    "                    model,\n",
    "                    use_task_type=use_task_type,\n",
    "                )\n",
    "\n",
    "                # get distance for each document embedding to the query embedding\n",
    "                distance_col = f\"{embedding_model_desc}_distance\"\n",
    "                df[distance_col] = df[embedding_model_desc].map(\n",
    "                    lambda x: round(get_distance(x, query_embedding), 2)\n",
    "                )\n",
    "\n",
    "                # get order of the distance\n",
    "                df[f\"{embedding_model_desc}_order\"] = (\n",
    "                    df[distance_col].rank().astype(int)\n",
    "                )\n",
    "            except InvalidArgument as e:\n",
    "                print(f\"Error while getting embeddings for {embedding_model_desc}: {e}\")\n",
    "                continue\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "COL_NAME = \"Semantic\"\n",
    "\n",
    "\n",
    "def create_result_file_from_csv(\n",
    "    csv_file: Path, use_task_type: bool, result_folder: Path\n",
    ") -> None:\n",
    "    # query is in file name\n",
    "    query = \" \".join(csv_file.stem.split(\"_\"))\n",
    "    print(f\"Processing {csv_file} with query: {query}\")\n",
    "\n",
    "    # load texts\n",
    "    docs = load_csv(csv_file, COL_NAME)\n",
    "\n",
    "    # get df with embeddings and distances to query\n",
    "    df = get_text_embedding_df(docs, query, EMBEDDING_MODELS)\n",
    "    df.to_parquet(result_folder / f\"{csv_file.stem}.parquet\", index=False)\n",
    "\n",
    "\n",
    "def create_clean_csv_from_result_file(result_file: Path) -> None:\n",
    "    df = pd.read_parquet(result_file)\n",
    "\n",
    "    # drop all columns that don't contain \"distance\" or \"order\"\n",
    "    df = df.loc[:, df.columns.str.contains(\"distance|order|result\")]\n",
    "\n",
    "    # copy index (+ 1) to new column \"rank\"\n",
    "    original_rank_col = \"original_rank\"\n",
    "    df[original_rank_col] = df.index + 1\n",
    "\n",
    "    # sort texts by each distance order (distance) column\n",
    "    for col in df.columns:\n",
    "        if col.endswith(\"_order\"):\n",
    "            result_col = col.replace(\"_order\", \"\")\n",
    "            distance_col = f\"{result_col}_distance\"\n",
    "            helper_df = df[\n",
    "                [\"result\", original_rank_col, col, distance_col]\n",
    "            ].sort_values(by=col)\n",
    "            df[result_col + \"_result\"] = helper_df[\"result\"].values\n",
    "            df[distance_col] = helper_df[distance_col].values\n",
    "            df[col] = helper_df[col].values\n",
    "\n",
    "    df = df.reindex(sorted(df.columns), axis=1)\n",
    "    df.to_csv(result_file.parent / (result_file.stem + \".csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../data/query_eval/Kinder_essen_in_deutschem_Kindergarten.csv with query: Kinder essen in deutschem Kindergarten\n",
      "Processing ../data/query_eval/Robert_Habeck_lachend.csv with query: Robert Habeck lachend\n",
      "Processing ../data/query_eval/Robert_Habeck_mit_positivem_Gesichtsausdruck,_z.B._lachend,_aber_ohne_Coronamaske.csv with query: Robert Habeck mit positivem Gesichtsausdruck, z.B. lachend, aber ohne Coronamaske\n",
      "Processing ../data/query_eval/Nachstellung_Jugendgewalt.csv with query: Nachstellung Jugendgewalt\n",
      "Processing ../data/query_eval/Menschen_trinken_Wasser.csv with query: Menschen trinken Wasser\n",
      "Processing ../data/query_eval/Olaf_Scholz_hält_Rede_im.csv with query: Olaf Scholz hält Rede im\n",
      "Processing ../data/query_eval/Fischmarkt_in_Hamburg_überflutet_nach_Sturmtief_Zoltan.csv with query: Fischmarkt in Hamburg überflutet nach Sturmtief Zoltan\n",
      "Processing ../data/query_eval/Robert_Habeck_lachen.csv with query: Robert Habeck lachen\n",
      "Processing ../data/query_eval/Fußgängerzone_im_Schnee.csv with query: Fußgängerzone im Schnee\n",
      "Processing ../data/query_eval/Porträt_Franz_Beckenbauer.csv with query: Porträt Franz Beckenbauer\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
