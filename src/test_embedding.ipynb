{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "from typing import Any, Optional\n",
    "from typing import Tuple\n",
    "import time\n",
    "\n",
    "from google.api_core.exceptions import InvalidArgument\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import vertexai\n",
    "from numpy import floating\n",
    "from vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\n",
    "\n",
    "DATA_FOLDER = Path(\"../data\")\n",
    "INPUT_FOLDER = DATA_FOLDER / \"query_eval\"\n",
    "RESULT_FOLDER = DATA_FOLDER / \"embedding_eval\"\n",
    "RESULT_TASKTYPE_FOLDER = (\n",
    "    DATA_FOLDER / \"embedding_eval_task_type\"\n",
    ")  # this contains the API call with type (QUERY or DOCUMENT)\n",
    "REGION = \"europe-west3\"\n",
    "PROJECT_ID = \"adesso-gcc-rtl-uc4\"\n",
    "EMBEDDING_MODELS = [\n",
    "    (\"gecko@001\", \"textembedding-gecko@001\"),\n",
    "    (\"gecko@002\", \"textembedding-gecko@002\"),\n",
    "    (\"gecko@003\", \"textembedding-gecko@003\"),\n",
    "    (\"gecko@00multilingual\", \"textembedding-gecko-multilingual@001\"),\n",
    "]\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(file: Path, col_name: str) -> list[str]:\n",
    "    with open(file, newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        return [\n",
    "            row[col_name].replace(\"ChromaDB:\\n\", \"\") for row in reader if row[col_name]\n",
    "        ]\n",
    "\n",
    "\n",
    "def rate_limit(max_per_minute):\n",
    "    period = 60 / max_per_minute\n",
    "    while True:\n",
    "        before = time.time()\n",
    "        yield\n",
    "        after = time.time()\n",
    "        elapsed = after - before\n",
    "        sleep_time = max(0, period - elapsed)\n",
    "        if sleep_time > 0:\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "\n",
    "def encode_texts_to_embeddings(\n",
    "    docs: list[str | TextEmbeddingInput],\n",
    "    embedding_model: TextEmbeddingModel,\n",
    "    use_task_type: bool,\n",
    "    instances_per_batch: int = 5,\n",
    "    requests_per_minute: int = 100,\n",
    ") -> list[Optional[list[float]]]:\n",
    "    if use_task_type:\n",
    "        docs = [\n",
    "            TextEmbeddingInput(text=sentence, task_type=\"RETRIEVAL_DOCUMENT\")  # type: ignore\n",
    "            for sentence in docs\n",
    "        ]\n",
    "    try:\n",
    "        embeddings = []\n",
    "        limiter = rate_limit(requests_per_minute)\n",
    "        while docs:\n",
    "            # Working in batches because the API accepts maximum 5\n",
    "            # documents per request to get embeddings\n",
    "            head, docs = (\n",
    "                docs[:instances_per_batch],\n",
    "                docs[instances_per_batch:],\n",
    "            )\n",
    "            chunk = embedding_model.get_embeddings(head)\n",
    "            embeddings.extend(chunk)\n",
    "            next(limiter)\n",
    "        return [embedding.values for embedding in embeddings]\n",
    "    except Exception as e:\n",
    "        print(f\"Error while getting embeddings: {e}\")\n",
    "        return [None for _ in range(len(docs))]\n",
    "\n",
    "\n",
    "def get_distance(document: list[float], query_embedding: list[float]) -> floating[Any]:\n",
    "    return np.linalg.norm(np.array(document) - np.array(query_embedding))\n",
    "\n",
    "\n",
    "def get_text_embedding_df(\n",
    "    docs: list[str],\n",
    "    query: str,\n",
    "    embedding_models: list[Tuple[str, str]],\n",
    "    use_task_type: bool,\n",
    ") -> pd.DataFrame:\n",
    "    df = pd.DataFrame()\n",
    "    df[\"result\"] = docs\n",
    "\n",
    "    for embedding_model_name, embedding_model in embedding_models:\n",
    "        try:\n",
    "            model = TextEmbeddingModel.from_pretrained(embedding_model)\n",
    "\n",
    "            # get query embedding\n",
    "            if use_task_type:\n",
    "                query_textinput = TextEmbeddingInput(\n",
    "                    text=query, task_type=\"RETRIEVAL_QUERY\"\n",
    "                )  # type: ignore\n",
    "                query_embedding = model.get_embeddings([query_textinput])[0].values\n",
    "            else:\n",
    "                query_embedding = model.get_embeddings([query])[0].values\n",
    "\n",
    "            # get text embeddings\n",
    "            df[embedding_model_name] = encode_texts_to_embeddings(\n",
    "                docs, model, use_task_type=use_task_type\n",
    "            )  # type: ignore\n",
    "\n",
    "            # get distance for each document embedding to the query embedding\n",
    "            distance_col = f\"{embedding_model_name}_distance\"\n",
    "            df[distance_col] = df[embedding_model_name].map(\n",
    "                lambda x: round(get_distance(x, query_embedding), 2)\n",
    "            )\n",
    "\n",
    "            # get order of the distance\n",
    "            df[f\"{embedding_model_name}_order\"] = df[distance_col].rank().astype(int)\n",
    "        except InvalidArgument as e:\n",
    "            print(f\"Error while getting embeddings for {embedding_model_name}: {e}\")\n",
    "            continue\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "COL_NAME = \"Semantic\"\n",
    "\n",
    "\n",
    "def create_evals(use_task_type: bool, result_folder: Path) -> None:\n",
    "    # get embeddings and distances\n",
    "    for csv_file in INPUT_FOLDER.glob(\"*.csv\"):\n",
    "        # query is in file name\n",
    "        query = \" \".join(csv_file.stem.split(\"_\"))\n",
    "        print(f\"Processing {csv_file} with query: {query}\")\n",
    "\n",
    "        # load texts\n",
    "        docs = load_csv(csv_file, COL_NAME)\n",
    "\n",
    "        # get df with embeddings and distances to query\n",
    "        df = get_text_embedding_df(\n",
    "            docs, query, EMBEDDING_MODELS, use_task_type=use_task_type\n",
    "        )\n",
    "        df.to_parquet(result_folder / f\"{csv_file.stem}.parquet\", index=False)\n",
    "\n",
    "    # clean up the CSV by removing embedding columns and re-ordering results\n",
    "    for result_file in result_folder.glob(\"*.parquet\"):\n",
    "        df = pd.read_parquet(result_file)\n",
    "\n",
    "        # drop all columns that don't contain \"distance\" or \"order\"\n",
    "        df = df.loc[:, df.columns.str.contains(\"distance|order|result\")]\n",
    "\n",
    "        # copy index (+ 1) to new column \"rank\"\n",
    "        original_rank_col = \"original_rank\"\n",
    "        df[original_rank_col] = df.index + 1\n",
    "\n",
    "        # sort texts by each distance order (distance) column\n",
    "        for col in df.columns:\n",
    "            if col.endswith(\"_order\"):\n",
    "                result_col = col.replace(\"_order\", \"\")\n",
    "                distance_col = f\"{result_col}_distance\"\n",
    "                helper_df = df[\n",
    "                    [\"result\", original_rank_col, col, distance_col]\n",
    "                ].sort_values(by=col)\n",
    "                df[result_col + \"_result\"] = helper_df[\"result\"].values\n",
    "                df[distance_col] = helper_df[distance_col].values\n",
    "                df[col] = helper_df[col].values\n",
    "\n",
    "        df = df.reindex(sorted(df.columns), axis=1)\n",
    "        df.to_csv(result_folder / (result_file.stem + \".csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../data/query_eval/Kinder_essen_in_deutschem_Kindergarten.csv with query: Kinder essen in deutschem Kindergarten\n",
      "Processing ../data/query_eval/Robert_Habeck_lachend.csv with query: Robert Habeck lachend\n",
      "Processing ../data/query_eval/Robert_Habeck_mit_positivem_Gesichtsausdruck,_z.B._lachend,_aber_ohne_Coronamaske.csv with query: Robert Habeck mit positivem Gesichtsausdruck, z.B. lachend, aber ohne Coronamaske\n",
      "Processing ../data/query_eval/Nachstellung_Jugendgewalt.csv with query: Nachstellung Jugendgewalt\n",
      "Processing ../data/query_eval/Menschen_trinken_Wasser.csv with query: Menschen trinken Wasser\n",
      "Processing ../data/query_eval/Olaf_Scholz_hält_Rede_im.csv with query: Olaf Scholz hält Rede im\n",
      "Processing ../data/query_eval/Fischmarkt_in_Hamburg_überflutet_nach_Sturmtief_Zoltan.csv with query: Fischmarkt in Hamburg überflutet nach Sturmtief Zoltan\n",
      "Processing ../data/query_eval/Robert_Habeck_lachen.csv with query: Robert Habeck lachen\n",
      "Processing ../data/query_eval/Fußgängerzone_im_Schnee.csv with query: Fußgängerzone im Schnee\n",
      "Processing ../data/query_eval/Porträt_Franz_Beckenbauer.csv with query: Porträt Franz Beckenbauer\n"
     ]
    }
   ],
   "source": [
    "create_evals(use_task_type=False, result_folder=RESULT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../data/query_eval/Kinder_essen_in_deutschem_Kindergarten.csv with query: Kinder essen in deutschem Kindergarten\n",
      "Error while getting embeddings for gecko@001: 400 Task type RETRIEVAL_QUERY requested, but this model version does not support task-specific inference.\n",
      "Processing ../data/query_eval/Robert_Habeck_lachend.csv with query: Robert Habeck lachend\n",
      "Error while getting embeddings for gecko@001: 400 Task type RETRIEVAL_QUERY requested, but this model version does not support task-specific inference.\n",
      "Processing ../data/query_eval/Robert_Habeck_mit_positivem_Gesichtsausdruck,_z.B._lachend,_aber_ohne_Coronamaske.csv with query: Robert Habeck mit positivem Gesichtsausdruck, z.B. lachend, aber ohne Coronamaske\n",
      "Error while getting embeddings for gecko@001: 400 Task type RETRIEVAL_QUERY requested, but this model version does not support task-specific inference.\n",
      "Processing ../data/query_eval/Nachstellung_Jugendgewalt.csv with query: Nachstellung Jugendgewalt\n",
      "Error while getting embeddings for gecko@001: 400 Task type RETRIEVAL_QUERY requested, but this model version does not support task-specific inference.\n",
      "Processing ../data/query_eval/Menschen_trinken_Wasser.csv with query: Menschen trinken Wasser\n",
      "Error while getting embeddings for gecko@001: 400 Task type RETRIEVAL_QUERY requested, but this model version does not support task-specific inference.\n",
      "Processing ../data/query_eval/Olaf_Scholz_hält_Rede_im.csv with query: Olaf Scholz hält Rede im\n",
      "Error while getting embeddings for gecko@001: 400 Task type RETRIEVAL_QUERY requested, but this model version does not support task-specific inference.\n",
      "Processing ../data/query_eval/Fischmarkt_in_Hamburg_überflutet_nach_Sturmtief_Zoltan.csv with query: Fischmarkt in Hamburg überflutet nach Sturmtief Zoltan\n",
      "Error while getting embeddings for gecko@001: 400 Task type RETRIEVAL_QUERY requested, but this model version does not support task-specific inference.\n",
      "Processing ../data/query_eval/Robert_Habeck_lachen.csv with query: Robert Habeck lachen\n",
      "Error while getting embeddings for gecko@001: 400 Task type RETRIEVAL_QUERY requested, but this model version does not support task-specific inference.\n",
      "Processing ../data/query_eval/Fußgängerzone_im_Schnee.csv with query: Fußgängerzone im Schnee\n",
      "Error while getting embeddings for gecko@001: 400 Task type RETRIEVAL_QUERY requested, but this model version does not support task-specific inference.\n",
      "Processing ../data/query_eval/Porträt_Franz_Beckenbauer.csv with query: Porträt Franz Beckenbauer\n",
      "Error while getting embeddings for gecko@001: 400 Task type RETRIEVAL_QUERY requested, but this model version does not support task-specific inference.\n"
     ]
    }
   ],
   "source": [
    "create_evals(use_task_type=True, result_folder=RESULT_TASKTYPE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
